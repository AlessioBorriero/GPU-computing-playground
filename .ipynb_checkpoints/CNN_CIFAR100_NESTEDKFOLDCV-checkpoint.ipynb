{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1e7ee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-29 15:56:14.591297: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-29 15:56:14.593072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 15:56:14.593290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 15:56:14.593436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 15:56:14.884075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 15:56:14.884268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 15:56:14.884415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 15:56:14.884560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38420 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                7850      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,960\n",
      "Trainable params: 7,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Training...\n",
      "\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 96% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-29 15:56:16.141456: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 24% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 36% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 39% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 37% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 38% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 36% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 39% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 36% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 35% | 97% |\n",
      "\n",
      "Testing in-sample...\n",
      "\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 29% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 15% | 97% |\n",
      "\n",
      "Testing out-of-sample...\n",
      "\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  5% | 97% |\n",
      "\n",
      "Accurcay over test set: 0.9003833333333333\n",
      "\n",
      "\n",
      "Training the model took 9.735613036900759 seconds\n",
      "\n",
      "\n",
      "Training duration summing batch processing time: 5.542431592941284 seconds\n",
      "\n",
      "\n",
      "Sample processing time during training: 10825.573395694166 sample/seconds\n",
      "\n",
      "\n",
      "Testing the model in-sample took 1.2133454009890556 seconds\n",
      "\n",
      "\n",
      "Sample processing time during inference in-sample: 49450.05762669982 sample/seconds\n",
      "\n",
      "\n",
      "Testing the model out-of-sample took 0.3265444729477167 seconds\n",
      "\n",
      "\n",
      "Sample processing time during inference out-of-sample: 30623.700072856867 sample/seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import time\n",
    "import sys\n",
    "import GPUtil\n",
    "from timeit import default_timer as timer\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from functools import reduce\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.metrics import accuracy_score\n",
    "from threading import Thread\n",
    "from numba import cuda\n",
    "\n",
    "\n",
    "input_shape = (1, 32, 32, 3)\n",
    "output_size = 100\n",
    "n_epochs = 100\n",
    "gpu_performance_sampling_time = 1\n",
    "gpu_performance_sampling_time_INFERENCE = 0.1\n",
    "\n",
    "# READ FROM COMMAND LINE DEVICE, NETWORK SIZE AND BATCH SIZE\n",
    "# CHECK GPU AVAILABILITY\n",
    "\n",
    "if tf.config.list_physical_devices('GPU') == 0:\n",
    "    print(\"GPU unavailable :(\")\n",
    "    sys.exit(0)\n",
    "\n",
    "# # READ ARGS FROM COMMAND LINE\n",
    "# # Raise error if correct arguments aren't given\n",
    "# if len(sys.argv) != 4:\n",
    "#     print(\"Matmul benchmark need 3 arguments:\")\n",
    "#     print(\"- Device\")\n",
    "#     print(\"- Shallow Layer dimension\")\n",
    "#     print(\"- Batch size\")\n",
    "#     sys.exit(1)\n",
    "\n",
    "\n",
    "# dev = sys.argv[1]\n",
    "# hidden_layer_list.append(int(sys.argv[2]))\n",
    "# batch_size = int(sys.argv[3])\n",
    "dev = 'gpu'\n",
    "batch_size = 1000\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "SET DEVICE\n",
    "\"\"\"\n",
    "if dev == 'cpu':\n",
    "    d = '/cpu:0'\n",
    "elif dev == 'gpu':\n",
    "    if tf.config.list_physical_devices('GPU') == 0:\n",
    "        print(\"GPU unavailable :(\")\n",
    "        sys.exit(0)\n",
    "    d = '/device:GPU:0'\n",
    "\n",
    "\"\"\"\n",
    "GPU USAGE MONITOR\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Monitor(Thread):\n",
    "\n",
    "    def __init__(self, delay):\n",
    "        super(Monitor, self).__init__()\n",
    "        self.stopped = False\n",
    "        self.delay = delay\n",
    "        self.start()\n",
    "\n",
    "    def run(self):\n",
    "        while not self.stopped:\n",
    "            GPUtil.showUtilization()\n",
    "            time.sleep(self.delay)\n",
    "\n",
    "    def stop(self):\n",
    "        self.stopped = True\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Timing callback definition\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.batch_times = []\n",
    "        self.epoch_times = []\n",
    "        self.training_time = []\n",
    "        self.training_time_start = time.time()\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        self.batch_time_start = time.time()\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.batch_times.append(time.time() - self.batch_time_start)\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.epoch_times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "    def on_train_end(self, batch, logs={}):\n",
    "        self.training_time.append(time.time() - self.training_time_start)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Data loading method\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def data_loading(output):\n",
    "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "    # Data preprocessing: I have to rescale and flatten all the images\n",
    "    shape = (1, 32, 32, 3)\n",
    "    # One-hot encoding\n",
    "    y_train = to_categorical(y_train, num_classes=output)\n",
    "    y_test = to_categorical(y_test, num_classes=output)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Model definition method\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def model_def(input_size, output):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (4,4), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1000, activation='relu', input_shape=(input_size,)))\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "    model.add(Dense(hidden_layer[i], activation='relu'))\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    optim = keras.optimizers.SGD(learning_rate=lr, momentum=0.05)\n",
    "    metrics = [\"accuracy\"]\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    (X_train, Y_train), (X_test, Y_test) = data_loading(output_size)\n",
    "    nn = model_def(hidden_layer_list, input_size, output_size)\n",
    "    nn.summary()\n",
    "\n",
    "    \"\"\"\n",
    "    Training\n",
    "    \"\"\"\n",
    "    time_callback = TimeHistory()\n",
    "    print(\"\\nTraining...\\n\")\n",
    "    nn = model_def(hidden_layer_list, input_size, output_size)\n",
    "    monitor = Monitor(gpu_performance_sampling_time)     # GPU MONITOR\n",
    "    begin = timer()  # Duration of the whole fit() method run\n",
    "    nn.fit(X_train, Y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "           callbacks=[time_callback], validation_split=0.3, verbose=0)\n",
    "    training_time = timer() - begin  # Duration of the whole fit() method run\n",
    "    monitor.stop()                                       # GPU MONITOR\n",
    "    training_time_sum_over_batches = sum(time_callback.batch_times)\n",
    "    time_per_sample = training_time_sum_over_batches/((len(X_train)//batch_size)\n",
    "                                                      * batch_size)\n",
    "    sample_per_second = 1./time_per_sample\n",
    "\n",
    "    \"\"\"\n",
    "    Testing In-Sample\n",
    "    \"\"\"\n",
    "    print(\"\\nTesting in-sample...\\n\")\n",
    "    monitor = Monitor(gpu_performance_sampling_time)     # GPU MONITOR\n",
    "    begin = timer()  # Inference time on training set\n",
    "    pred = nn.predict(X_train).argmax(1)\n",
    "    testing_time_insample = timer() - begin  # Inference time on training set\n",
    "    monitor.stop()                                       # GPU MONITOR\n",
    "    accuracy = accuracy_score(Y_train.argmax(1),\n",
    "                              pred, normalize=False)/len(X_train)\n",
    "    time_per_sample_test_insample = testing_time_insample/len(X_train)\n",
    "    sample_per_second_test_insample = 1./time_per_sample_test_insample\n",
    "\n",
    "    \"\"\"\n",
    "    Testing Out-of-Sample\n",
    "    \"\"\"\n",
    "    print(\"\\nTesting out-of-sample...\\n\")\n",
    "    monitor = Monitor(gpu_performance_sampling_time)     # GPU MONITOR\n",
    "    begin = timer()  # Inference time on training set\n",
    "    pred = nn.predict(X_test).argmax(1)\n",
    "    testing_time_outofsample = timer() - begin  # Inference time on training set\n",
    "    monitor.stop()                                       # GPU MONITOR\n",
    "    accuracy_score(Y_test.argmax(1),\n",
    "                   pred, normalize=False)/len(X_test)\n",
    "    time_per_sample_test_outofsample = testing_time_outofsample/len(X_test)\n",
    "    sample_per_second_test_outofsample = 1./time_per_sample_test_outofsample\n",
    "\n",
    "    \"\"\"\n",
    "    free gpu memory\n",
    "    \"\"\"\n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()\n",
    "\n",
    "    print(\"\\nAccurcay over test set: %s\\n\" % accuracy)\n",
    "\n",
    "    print(\"\\nTraining the model took %s seconds\\n\" % training_time)\n",
    "\n",
    "    print(\"\\nTraining duration summing batch processing time: %s seconds\\n\"\n",
    "          % training_time_sum_over_batches)\n",
    "\n",
    "    print(\"\\nSample processing time during training: %s sample/seconds\\n\"\n",
    "          % sample_per_second)\n",
    "\n",
    "    print(\"\\nTesting the model in-sample took %s seconds\\n\" % testing_time_insample)\n",
    "\n",
    "    print(\"\\nSample processing time during inference in-sample: %s sample/seconds\\n\"\n",
    "          % sample_per_second_test_insample)\n",
    "\n",
    "    print(\"\\nTesting the model out-of-sample took %s seconds\\n\"\n",
    "          % testing_time_outofsample)\n",
    "\n",
    "    print(\"\\nSample processing time during inference out-of-sample: %s sample/seconds\\n\"\n",
    "          % sample_per_second_test_outofsample)\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with tf.device(dev):\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
