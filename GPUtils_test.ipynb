{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "397b94ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPUtil\n",
    "from threading import Thread\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import time\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from functools import reduce\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ac7981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deviceIDs = GPUtil.getAvailable(order = 'first', limit = 1, maxLoad = 0.5, maxMemory = 0.5,\n",
    "                                includeNan=False, excludeID=[], excludeUUID=[])\n",
    "deviceIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f82502d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPUs = GPUtil.getGPUs()\n",
    "GPUtil.getAvailability(GPUs, maxLoad = 0.5, maxMemory = 0.5, includeNan=False, excludeID=[], excludeUUID=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af7e2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Monitor(Thread):\n",
    "    def __init__(self, delay):\n",
    "        super(Monitor, self).__init__()\n",
    "        self.stopped = False\n",
    "        self.delay = delay # Time between calls to GPUtil\n",
    "        self.start()\n",
    "\n",
    "    def run(self):\n",
    "        while not self.stopped:\n",
    "            GPUtil.showUtilization()\n",
    "            time.sleep(self.delay)\n",
    "\n",
    "    def stop(self):\n",
    "        self.stopped = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1799849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-28 19:21:33.511481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-28 19:21:33.539362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-28 19:21:33.539565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-28 19:21:33.541068: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-28 19:21:33.542135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-28 19:21:33.542291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-28 19:21:33.542431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-28 19:21:33.831900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-28 19:21:33.832097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-28 19:21:33.832245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-28 19:21:33.832385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38420 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1024)              803840    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                10250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814,090\n",
      "Trainable params: 814,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Training...\n",
      "\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 96% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-28 19:21:35.111638: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 21% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 39% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 38% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 39% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 38% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 38% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 38% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 33% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 40% | 97% |\n",
      "\n",
      "Testing in-sample...\n",
      "\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 40% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 23% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 15% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 14% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 13% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 13% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 14% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 15% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 16% | 97% |\n",
      "\n",
      "Testing out-of-sample...\n",
      "\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 16% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  4% | 97% |\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 10% | 97% |\n",
      "\n",
      "Training the model took 10.308788103051484 seconds\n",
      "\n",
      "Sample processing time during training: 10158.225376373844 sample/seconds\n",
      "\n",
      "Training duration summing batch processing time: 5.906543493270874 seconds\n",
      "\n",
      "Testing the model in-sample took 1.411790071055293 seconds\n",
      "\n",
      "Sample processing time during inference in-sample: 42499.236416325584 sample/seconds\n",
      "\n",
      "Testing the model out-of-sample took 0.37902499409392476 seconds\n",
      "\n",
      "Sample processing time during inference out-of-sample: 26383.4843501691 sample/seconds\n"
     ]
    }
   ],
   "source": [
    "input_size = 28*28\n",
    "output_size = 10\n",
    "n_epochs = 100\n",
    "gpu_performance_sampling_time = 1\n",
    "gpu_performance_sampling_time_INFERENCE = 0.1\n",
    "\n",
    "\n",
    "# READ FROM COMMAND LINE DEVICE, NETWORK SIZE AND BATCH SIZE\n",
    "\n",
    "# CHECK GPU AVAILABILITY\n",
    "\n",
    "if tf.config.list_physical_devices('GPU') == 0:\n",
    "    print(\"GPU unavailable :(\")\n",
    "    sys.exit(0)\n",
    "\n",
    "\n",
    "hidden_layer_list = []  # This list is read from the settings file\n",
    "\n",
    "dev = 'gpu'\n",
    "hidden_layer_list.append(1024)\n",
    "batch_size = int(1000)\n",
    "\n",
    "# SET DEVICE\n",
    "\n",
    "if dev == 'cpu':\n",
    "    d = '/cpu:0'\n",
    "elif dev == 'gpu':\n",
    "    if tf.config.list_physical_devices('GPU') == 0:\n",
    "        print(\"GPU unavailable :(\")\n",
    "        sys.exit(0)\n",
    "    d = '/device:GPU:0'\n",
    "\n",
    "\n",
    "# Timing callback definition\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.batch_times = []\n",
    "        self.epoch_times = []\n",
    "        self.training_time = []\n",
    "        self.training_time_start = time.time()\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        self.batch_time_start = time.time()\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.batch_times.append(time.time() - self.batch_time_start)\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.epoch_times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "    def on_train_end(self, batch, logs={}):\n",
    "        self.training_time.append(time.time() - self.training_time_start)\n",
    "\n",
    "\n",
    "# Data loading method\n",
    "def data_loading(output):\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    # Data preprocessing: I have to rescale and flatten all the images\n",
    "    shape = (28, 28)\n",
    "    shape_l = reduce(lambda a, b: a*b, shape)\n",
    "    x_train = x_train.reshape((-1, shape_l)) / 255.\n",
    "    x_test = x_test.reshape((-1, shape_l)) / 255.\n",
    "    # One-hot encoding\n",
    "    y_train = to_categorical(y_train, num_classes=output)\n",
    "    y_test = to_categorical(y_test, num_classes=output)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "# Model defintion method\n",
    "def model_def(hidden_layer, input, output):\n",
    "    model = Sequential()\n",
    "    for i in range(len(hidden_layer)+1):\n",
    "        if i == 0:\n",
    "            model.add(Dense(hidden_layer[i], activation='relu',\n",
    "                      input_shape=(input_size,)))\n",
    "        elif i == len(hidden_layer):\n",
    "            model.add(Dense(output_size, activation='softmax'))\n",
    "        else:\n",
    "            model.add(Dense(hidden_layer[i], activation='relu'))\n",
    "    loss = keras.losses.CategoricalCrossentropy()\n",
    "    optim = keras.optimizers.SGD(learning_rate=0.01, momentum=0.05)\n",
    "    metrics = [\"accuracy\"]\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    (X_train, Y_train), (X_test, Y_test) = data_loading(output_size)\n",
    "    nn = model_def(hidden_layer_list, input_size, output_size)\n",
    "    nn.summary()\n",
    "\n",
    "    \n",
    "    # TRAINING\n",
    "\n",
    "    time_callback = TimeHistory()\n",
    "    print(\"\\nTraining...\\n\")\n",
    "    nn = model_def(hidden_layer_list, input_size, output_size)\n",
    "    monitor = Monitor(gpu_performance_sampling_time)\n",
    "    begin = timer()  # Duration of the whole fit() method run\n",
    "    nn.fit(X_train, Y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "           callbacks=[time_callback], validation_split=0.3, verbose=0)\n",
    "    training_time = timer() - begin  # Duration of the whole fit() method run\n",
    "    monitor.stop()\n",
    "    training_time_sum_over_batches = sum(time_callback.batch_times)\n",
    "    time_per_sample = training_time_sum_over_batches/((len(X_train)//batch_size)\n",
    "                                                      * batch_size)\n",
    "    sample_per_second = 1./time_per_sample\n",
    "    \n",
    "    \n",
    "    \n",
    "    # TESTING IN-SAMPLE\n",
    "\n",
    "    print(\"\\nTesting in-sample...\\n\")\n",
    "    # Evaluate the model in-sample\n",
    "    monitor = Monitor(gpu_performance_sampling_time_INFERENCE)  ## GPU MONITOR\n",
    "    begin = timer()  # Inference time on training set\n",
    "    pred = nn.predict(X_train).argmax(1)\n",
    "    testing_time_insample = timer() - begin  # Inference time on training set\n",
    "    monitor.stop()                                    ## GPU MONITOR\n",
    "    accuracy_score(Y_train.argmax(1),\n",
    "                   pred, normalize=False)/len(X_train)\n",
    "    time_per_sample_test_insample = testing_time_insample/len(X_train)\n",
    "    sample_per_second_test_insample = 1./time_per_sample_test_insample\n",
    "\n",
    "    # TESTING OUT-OF-SAMPLE\n",
    "\n",
    "    print(\"\\nTesting out-of-sample...\\n\")\n",
    "    # Evaluate the model out-of-sample\n",
    "    monitor = Monitor(gpu_performance_sampling_time_INFERENCE)  ## GPU MONITOR\n",
    "    begin = timer()  # Inference time on training set\n",
    "    pred = nn.predict(X_test).argmax(1)\n",
    "    testing_time_outofsample = timer() - begin  # Inference time on training set\n",
    "    monitor.stop()                                    ## GPU MONITOR\n",
    "    # test_accuracy_out_of_sample = accuracy_score(Y_test.argmax(1),\n",
    "    #                                              pred, normalize=False)/len(X_test)\n",
    "    accuracy_score(Y_test.argmax(1),\n",
    "                   pred, normalize=False)/len(X_test)\n",
    "    time_per_sample_test_outofsample = testing_time_outofsample/len(X_test)\n",
    "    sample_per_second_test_outofsample = 1./time_per_sample_test_outofsample\n",
    "    \n",
    "    \"\"\"\n",
    "    free gpu memory\n",
    "    \"\"\"\n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()\n",
    "\n",
    "    print(\"\\nTraining the model took %s seconds\\n\" % training_time)\n",
    "\n",
    "    print(\"Sample processing time during training: %s sample/second\\n\"\n",
    "          % sample_per_second)\n",
    "\n",
    "    print(\"Training duration summing batch processing time: %s second\\n\"\n",
    "          % training_time_sum_over_batches)\n",
    "\n",
    "    print(\"Testing the model in-sample took %s second\\n\" % testing_time_insample)\n",
    "\n",
    "    print(\"Sample processing time during inference in-sample: %s sample/second\\n\"\n",
    "          % sample_per_second_test_insample)\n",
    "\n",
    "    print(\"Testing the model out-of-sample took %s second\\n\" % testing_time_outofsample)\n",
    "\n",
    "    print(\"Sample processing time during inference out-of-sample: %s sample/second\"\n",
    "          % sample_per_second_test_outofsample)\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with tf.device(dev):\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
